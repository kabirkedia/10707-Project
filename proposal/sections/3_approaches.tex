% !TEX root=../proposal.tex

\section{Approach}
\label{sec:approach}

\subsection{Deep Deterministic Policy Gradient (DDPG)}
DDPG is an actor critic method that adapts the underyling success of Deep Q-learning 
to the continuous action domain. DDPG itself is based on the deterministic
policy gradient algorithm (DPG)~\cite{silver2014deterministic} which rewrites the gradient
of the objective $J(\theta)$(Section 2.3) as:
\begin{equation}
\nabla_{\theta}J(\theta) = E_{s \sim D} [\nabla_{\theta}\mu_{\theta}(a|s)\nabla_{a} Q^{\mu}(s,a)|_{a = \mu_{\theta}(s)}]
\end{equation}
where $\mu_{\theta}: S -> A$ are deterministic policies. DDPG approximates
both the policy $\mu$ and the critic $Q^{\mu}$ with deep neural networks, sampling
trajectories from a replay buffer of experiences and using a target network as 
in DQN. Note that in the multi agent setting, each agent is trained individually
with DDPG, there is no combined optimization of the agents. However, the MADDPG 
algorithm, explained in the next section, naturally extends DDPG to the multi-agent
setting during training, resulting in much richer behavior between agents. 

\subsection{Multi-Agent Actor Critic for Mixed Cooperative-Competitive Environments (MADDPG)}
MADDPG is a recently developed general-purpose multi-agent learning algorithm
that is a simple extension of actor-critic policy gradient methods (specifically DDPG) 
where the critic is augmented with extra information about the policies of other 
agents while the actor only has access to local information (i.e., its own
observations). In this framework of centralized training with decentralized
execution, agents don’t need to access the central critic at test time; they
learn approximate models of other agents and effectively use them in their own
policy learning procedure. Furthermore, since the centralized critic is
learned independently for each agent, this approach can be used to model arbitrary 
reward structures between agents, including adversarial cases where the rewards are opposing.

More concretely, consider a game with N agents with policies parameterized by 
$\theta = {\theta_1, ..., \theta_N}$ and let $\mu = {\mu_1, ..., \mu_N}$ be the set of 
all agent policies. Then, the gradient of the expected return for agent i with deterministic 
policies $\mu_{\theta_i}$ is written as
\begin{equation}
\nabla_{\theta_i}J(\mu_i) = E_{x, a \sim D}[\nabla_{\theta_i}\mu_i(a_i|o_i)\nabla_{a_i}Q_i^{\mu}(x, a_1, ..., a_N)|_{a_i = \mu_i(o_i)}]
\end{equation}
Here the experience replay buffer D contains the tuples $(x, x_0, a_1,..., a_N, r_1,..., r_N)$, 
recording experiences of all agents. $Q^{\mu}_i$ is the centralized action-value function 
that takes as input the actions of all agents, in addition to some state information x, and outputs
the Q-value for agent i. Since each $Q^{\mu}_i$ is learned separately, agents can have 
arbitrary reward structures, including conflicting rewards in a competitive setting.  

A primary motivation behind MADDPG is that, if the actions taken by all the agents are known, 
then the environment is stationary as the policies change. This is not the case if the actions 
of other agents are not explicity conditioned on, as done for most traditional RL methods. 
Finally, note that the policies of other agents needs to be known to compute the loss. Knowing the 
observations and policies of other agents is not a particularly restrictive assumption; 
if the goal is to train agents to exhibit complex communicative behaviour in simulation, 
this information is often available to all agents. However, MADDPG also allows for
this assumption to be relaxed by learning the policies of other agents from observations.

\subsection{Evolutionary Approach to Multi Agent Systems}
Finally, we investigate certain optimizations in training such as
evolutionary strategies, which have been shown to be competitive in training
to traditional methods while allowing increased
parallelism~\cite{salimans2017evolution}.
Increased parallelism and scalability are important for allowing larger models to be trained~\cite{nair2015massively}.
We investigate both algorithmic and computer systems optimizations which may make this a viable training technique.
Namely, we explore how the quantity and quality of updates affect training performance.
If these methods prove successful, they can be incorporated with other training methods to increase learning scalability.

We use evolutionary techniques to brute force multi-agent training.
The main idea is to see if the quantity of updates prevails over the quality over updates.
We are equipped to solve this problem as we have access to a cluster with over $300$ nodes with $4$ cores per node.
This yields over $1200$ cores total, which is on the same order of magnitude as the cluster sizes used for state of the art research~\cite{salimans2017evolution}.
There are many metrics worth considering here, but the main ones will measure learning progress per unit time.
This is because for every update a traditional algorithm will due, the evolutionary strategy can do thousands.

One avenue worth exploring is whether this sort of problem can be optimized and deployed effectively over GPUs.
Evolutionary algorithms are embarassingly parallel, but it's not necessarily true that they will run well on GPUs.
Furter, it’s may be worth investigating how parameter sharing impacts an evolutionary search.
Sharing parameters decreases the complexity of the learning task, however it increases the required communication required in the system.
Allowing parameters to grow stale may help counteract some of these negative effects~\cite{cui2014exploiting}.
